{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a725a3ff-e627-45c7-ac77-ebbe3ef13c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12.2 MB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.4-py2.py3-none-any.whl (500 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500 kB 90.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.20.3\n",
      "  Downloading numpy-1.23.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.1 MB 30.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, numpy, pandas\n",
      "Successfully installed numpy-1.23.3 pandas-1.5.0 pytz-2022.4\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "900c08e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c12b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\n",
    "    service_name='s3',\n",
    "    region_name='us-west-2',\n",
    "    aws_access_key_id='***REMOVED***',\n",
    "    aws_secret_access_key='***REMOVED***'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa40ed06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 Âµs, sys: 1e+03 ns, total: 3 Âµs\n",
      "Wall time: 5.96 Âµs\n",
      "Successful S3 get_object response. Status - 200\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "response = s3.get_object(Bucket='reddit-title-generation', Key=\"dataset-train.csv\")\n",
    "\n",
    "status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "if status == 200:\n",
    "    print(f\"Successful S3 get_object response. Status - {status}\")\n",
    "    train = pd.read_csv(response.get(\"Body\"))\n",
    "    print(df)\n",
    "else:\n",
    "    print(f\"Unsuccessful S3 get_object response. Status - {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66b6b98c-2615-49e7-ab0f-902edcbaa0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = s3.select_object_content(\n",
    "    Bucket='reddit-title-generation',\n",
    "    Key='dataset-train.csv',\n",
    "    ExpressionType='SQL',\n",
    "    Expression=\"SELECT * FROM s3object S3Object LIMIT 1000\",\n",
    "    InputSerialization = {'CSV': {\"FileHeaderInfo\": \"None\", 'AllowQuotedRecordDelimiter':True}, 'CompressionType': 'NONE'},\n",
    "    OutputSerialization = {'CSV': {}},\n",
    ")\n",
    "\n",
    "records = []\n",
    "for event in resp['Payload']:\n",
    "    if 'Records' in event:\n",
    "        # records.append(event['Records']['Payload'].decode('utf-8'))\n",
    "        records.append(event['Records']['Payload'])  \n",
    "        \n",
    "file_str = ''.join(req.decode('utf-8') for req in records)\n",
    "    # elif 'Stats' in event:\n",
    "    #     statsDetails = event['Stats']['Details']\n",
    "    #     print(\"Stats details bytesScanned: \")\n",
    "    #     print(statsDetails['BytesScanned'])\n",
    "    #     print(\"Stats details bytesProcessed: \")\n",
    "    #     print(statsDetails['BytesProcessed'])\n",
    "    #     print(\"Stats details bytesReturned: \")\n",
    "    #     print(statsDetails['BytesReturned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ecc7521-9b89-41d9-9bd4-6b708e0477dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>normalizedBody</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2099088</td>\n",
       "      <td>UghImRegistered</td>\n",
       "      <td>&amp;gt; â€œI have friends with the same degree as m...</td>\n",
       "      <td>&gt; â€œI have friends with the same degree as me, ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>t5_2cneq</td>\n",
       "      <td>c1v1gu3</td>\n",
       "      <td>I have friends with the same degree as me, fro...</td>\n",
       "      <td>co-op, get some.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2893884</td>\n",
       "      <td>PossibleLesbian</td>\n",
       "      <td>Just a bit of background: I grew up Catholic. ...</td>\n",
       "      <td>Just a bit of background: I grew up Catholic. ...</td>\n",
       "      <td>actuallesbians</td>\n",
       "      <td>t5_2rch0</td>\n",
       "      <td>t3_185lqh</td>\n",
       "      <td>Just a bit of background: I grew up Catholic. ...</td>\n",
       "      <td>Former Catholic confused about sexuality. Has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2237635</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>I myself enjoy approaching an attractive young...</td>\n",
       "      <td>I myself enjoy approaching an attractive young...</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_g0rk6</td>\n",
       "      <td>I myself enjoy approaching an attractive young...</td>\n",
       "      <td>I've noticed a lot of stuff on Reddit concerni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>498777</td>\n",
       "      <td>Naztash</td>\n",
       "      <td>You do realize that the contract probably has ...</td>\n",
       "      <td>You do realize that the contract probably has ...</td>\n",
       "      <td>TopGear</td>\n",
       "      <td>t5_2r9n6</td>\n",
       "      <td>cpcdljw</td>\n",
       "      <td>You do realize that the contract probably has ...</td>\n",
       "      <td>He is not their child, but he is acting like a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1337130</td>\n",
       "      <td>BurChaBow</td>\n",
       "      <td>[](/dashiewilliamisboredofnamingemotes)\\n\\nI g...</td>\n",
       "      <td>ï¿¿ I got a teacher that used the most ridiculou...</td>\n",
       "      <td>MLPLounge</td>\n",
       "      <td>t5_2t403</td>\n",
       "      <td>cgej4pd</td>\n",
       "      <td>I got a teacher that used the most ridiculous ...</td>\n",
       "      <td>Teacher likes papers, and said  \"Pdf isn't the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>443715</td>\n",
       "      <td>bethechangeyouwant</td>\n",
       "      <td>I've been discussing an idea at length in whic...</td>\n",
       "      <td>I've been discussing an idea at length in whic...</td>\n",
       "      <td>explainlikeimfive</td>\n",
       "      <td>t5_2sokd</td>\n",
       "      <td>crf8qbu</td>\n",
       "      <td>I've been discussing an idea at length in whic...</td>\n",
       "      <td>All things exist in a constant \"now\", always h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>1711484</td>\n",
       "      <td>freedod</td>\n",
       "      <td>I don't see why people do this shit for fun. L...</td>\n",
       "      <td>I don't see why people do this shit for fun. L...</td>\n",
       "      <td>leagueoflegends</td>\n",
       "      <td>t5_2rfxx</td>\n",
       "      <td>cedo4hq</td>\n",
       "      <td>I don't see why people do this shit for fun. L...</td>\n",
       "      <td>if you're going to fuck with people, at least ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>375071</td>\n",
       "      <td>mbitr</td>\n",
       "      <td>These people are giving you solid advice. If y...</td>\n",
       "      <td>These people are giving you solid advice. If y...</td>\n",
       "      <td>army</td>\n",
       "      <td>t5_2qtr8</td>\n",
       "      <td>cgsgfgd</td>\n",
       "      <td>These people are giving you solid advice. If y...</td>\n",
       "      <td>You're headed down the wrong path and have unr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>2254985</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>I did mushrooms about a year ago when I was ba...</td>\n",
       "      <td>I did mushrooms about a year ago when I was ba...</td>\n",
       "      <td>Drugs</td>\n",
       "      <td>t5_2qh7l</td>\n",
       "      <td>t3_riaux</td>\n",
       "      <td>I did mushrooms about a year ago when I was ba...</td>\n",
       "      <td>Had one bad trip, will it affect any future tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>211696</td>\n",
       "      <td>DasGanon</td>\n",
       "      <td>The problem is theory, and manufacture.... let...</td>\n",
       "      <td>The problem is theory, and manufacture.... let...</td>\n",
       "      <td>AdviceAnimals</td>\n",
       "      <td>t5_2s7tt</td>\n",
       "      <td>cf2omu4</td>\n",
       "      <td>The problem is theory, and manufacture.... let...</td>\n",
       "      <td>It's not the 4 wheel drive that's throwing peo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1028 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0              author  \\\n",
       "0       2099088     UghImRegistered   \n",
       "1       2893884     PossibleLesbian   \n",
       "2       2237635           [deleted]   \n",
       "3        498777             Naztash   \n",
       "4       1337130           BurChaBow   \n",
       "...         ...                 ...   \n",
       "1023     443715  bethechangeyouwant   \n",
       "1024    1711484             freedod   \n",
       "1025     375071               mbitr   \n",
       "1026    2254985           [deleted]   \n",
       "1027     211696            DasGanon   \n",
       "\n",
       "                                                   body  \\\n",
       "0     &gt; â€œI have friends with the same degree as m...   \n",
       "1     Just a bit of background: I grew up Catholic. ...   \n",
       "2     I myself enjoy approaching an attractive young...   \n",
       "3     You do realize that the contract probably has ...   \n",
       "4     [](/dashiewilliamisboredofnamingemotes)\\n\\nI g...   \n",
       "...                                                 ...   \n",
       "1023  I've been discussing an idea at length in whic...   \n",
       "1024  I don't see why people do this shit for fun. L...   \n",
       "1025  These people are giving you solid advice. If y...   \n",
       "1026  I did mushrooms about a year ago when I was ba...   \n",
       "1027  The problem is theory, and manufacture.... let...   \n",
       "\n",
       "                                         normalizedBody          subreddit  \\\n",
       "0     > â€œI have friends with the same degree as me, ...           politics   \n",
       "1     Just a bit of background: I grew up Catholic. ...     actuallesbians   \n",
       "2     I myself enjoy approaching an attractive young...          AskReddit   \n",
       "3     You do realize that the contract probably has ...            TopGear   \n",
       "4     ï¿¿ I got a teacher that used the most ridiculou...          MLPLounge   \n",
       "...                                                 ...                ...   \n",
       "1023  I've been discussing an idea at length in whic...  explainlikeimfive   \n",
       "1024  I don't see why people do this shit for fun. L...    leagueoflegends   \n",
       "1025  These people are giving you solid advice. If y...               army   \n",
       "1026  I did mushrooms about a year ago when I was ba...              Drugs   \n",
       "1027  The problem is theory, and manufacture.... let...      AdviceAnimals   \n",
       "\n",
       "     subreddit_id         id  \\\n",
       "0        t5_2cneq    c1v1gu3   \n",
       "1        t5_2rch0  t3_185lqh   \n",
       "2        t5_2qh1i   t3_g0rk6   \n",
       "3        t5_2r9n6    cpcdljw   \n",
       "4        t5_2t403    cgej4pd   \n",
       "...           ...        ...   \n",
       "1023     t5_2sokd    crf8qbu   \n",
       "1024     t5_2rfxx    cedo4hq   \n",
       "1025     t5_2qtr8    cgsgfgd   \n",
       "1026     t5_2qh7l   t3_riaux   \n",
       "1027     t5_2s7tt    cf2omu4   \n",
       "\n",
       "                                                content  \\\n",
       "0     I have friends with the same degree as me, fro...   \n",
       "1     Just a bit of background: I grew up Catholic. ...   \n",
       "2     I myself enjoy approaching an attractive young...   \n",
       "3     You do realize that the contract probably has ...   \n",
       "4     I got a teacher that used the most ridiculous ...   \n",
       "...                                                 ...   \n",
       "1023  I've been discussing an idea at length in whic...   \n",
       "1024  I don't see why people do this shit for fun. L...   \n",
       "1025  These people are giving you solid advice. If y...   \n",
       "1026  I did mushrooms about a year ago when I was ba...   \n",
       "1027  The problem is theory, and manufacture.... let...   \n",
       "\n",
       "                                                summary  \n",
       "0                                      co-op, get some.  \n",
       "1     Former Catholic confused about sexuality. Has ...  \n",
       "2     I've noticed a lot of stuff on Reddit concerni...  \n",
       "3     He is not their child, but he is acting like a...  \n",
       "4     Teacher likes papers, and said  \"Pdf isn't the...  \n",
       "...                                                 ...  \n",
       "1023  All things exist in a constant \"now\", always h...  \n",
       "1024  if you're going to fuck with people, at least ...  \n",
       "1025  You're headed down the wrong path and have unr...  \n",
       "1026  Had one bad trip, will it affect any future tr...  \n",
       "1027  It's not the 4 wheel drive that's throwing peo...  \n",
       "\n",
       "[1028 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import StringIO\n",
    "train = pd.read_csv(StringIO(file_str), header=0)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14a292cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow -q\n",
    "!pip install transformers -q\n",
    "!pip install ohmeow-blurr -q\n",
    "!pip install bert-score -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711cd8d2",
   "metadata": {},
   "source": [
    "## BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7358264d-72d1-4a76-93f7-3b4babc2fbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving 0 files to the new cache system\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299d54ea6fd4486ba9dbb8f9a7317d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-07 22:42:23.473535: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-07 22:42:24.484043: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-07 22:42:24.484101: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-07 22:42:24.601501: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-07 22:42:26.365823: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-07 22:42:26.366020: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-07 22:42:26.366031: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/blurr/text/modeling/question_answering.py:31: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  squad_metric = load_metric(\"squad\")\n"
     ]
    }
   ],
   "source": [
    "from fastai.text.all import *\n",
    "from transformers import *\n",
    "from blurr.text.data.all import *\n",
    "from blurr.text.modeling.all import *\n",
    "\n",
    "#Select part of data we want to keep\n",
    "train_texts = train[['content','summary']]\n",
    "\n",
    "#Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc0c6fdb-b2a4-4bbc-9aea-d0e2aae185be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/blurr/text/data/seq2seq/summarization.py:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  final_df = final_df.append(self._process_df_batch(batch_df))\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/blurr/text/data/seq2seq/summarization.py:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  final_df = final_df.append(self._process_df_batch(batch_df))\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_name = \"facebook/bart-large-cnn\"\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(pretrained_model_name, \n",
    "                                                                  model_cls=BartForConditionalGeneration)\n",
    "\n",
    "# Create mini-batch and define parameters\n",
    "hf_batch_tfm = Seq2SeqBatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model, \n",
    "    task='summarization')\n",
    "\n",
    "# Simple preprocessing\n",
    "preprocessor = SummarizationPreprocessor(\n",
    "    hf_tokenizer,\n",
    "    text_attr='content',\n",
    "    target_text_attr='summary',\n",
    "    max_input_tok_length=256,\n",
    "    max_target_tok_length=130,\n",
    "    min_summary_char_length=30,\n",
    ")\n",
    "\n",
    "preprocessed_train = preprocessor.process_df(train_texts)\n",
    "\n",
    "\n",
    "# Prepare data for training\n",
    "blocks = (Seq2SeqTextBlock(batch_tokenize_tfm=hf_batch_tfm), noop)\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader('content'), get_y=ColReader('summary'), splitter=RandomSplitter())\n",
    "dls = dblock.dataloaders(preprocessed_train, bs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f4f06-9e5c-4885-94a0-8d6ffefe210d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/3 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>bertscore_precision</th>\n",
       "      <th>bertscore_recall</th>\n",
       "      <th>bertscore_f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='10' class='' max='358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      2.79% [10/358 00:52&lt;30:38 4.2775]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Define performance metrics\n",
    "seq2seq_metrics = {\n",
    "        'rouge': {\n",
    "            'compute_kwargs': { 'rouge_types': [\"rouge1\", \"rouge2\", \"rougeL\"], 'use_stemmer': True },\n",
    "            'returns': [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    "        },\n",
    "        'bertscore': {\n",
    "            'compute_kwargs': { 'lang': 'fr' },\n",
    "            'returns': [\"precision\", \"recall\", \"f1\"]}}\n",
    "\n",
    "#Model\n",
    "model = BaseModelWrapper(hf_model)\n",
    "learn_cbs = [BaseModelCallback]\n",
    "fit_cbs = [Seq2SeqMetricsCallback(custom_metrics=seq2seq_metrics)]\n",
    "\n",
    "#Specify training\n",
    "learn = Learner(dls, model,\n",
    "                opt_func=ranger,loss_func=CrossEntropyLossFlat(),\n",
    "                cbs=learn_cbs,splitter=partial(blurr_seq2seq_splitter, arch=hf_arch)).to_fp16()\n",
    "\n",
    "#Create optimizer with default hyper-parameters\n",
    "learn.create_opt() \n",
    "learn.freeze()\n",
    "\n",
    "#Training\n",
    "learn.fit_one_cycle(3, lr_max=3e-5, cbs=fit_cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479dee21-f75c-454b-a52e-65b4b0b0f31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = learn.blurr_generate(text_to_generate, early_stopping=False, num_return_sequences=1)\n",
    "\n",
    "for idx, o in enumerate(outputs):\n",
    "    print(f'=== Prediction {idx+1} ===\\n{o}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dd5d95",
   "metadata": {},
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1beed3f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d96ce8d16ed49cf824133315d008227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b38e2a65b9341f0bf52c956eaa9718b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89232da966554b1da2913e3928296e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c7d0d84447428b8020aa520f9600a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "t5_summarizer = pipeline(\"summarization\", model=\"t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "000dfa4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'coop programs are a great way to gain real-world work experience . Kyle bishop, 23, graduated from the university of Pittsburgh with 2 years of work experience in his field . he has been hired for two jobs that pay 65k+ during the tail end of a recession .'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_summarizer(train.content[0], min_length=5, max_length=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
