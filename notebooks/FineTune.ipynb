{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73b0e43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.24.89-py3-none-any.whl (132 kB)\n",
      "     -------------------------------------- 132.5/132.5 kB 3.8 MB/s eta 0:00:00\n",
      "Collecting botocore<1.28.0,>=1.27.89\n",
      "  Using cached botocore-1.27.89-py3-none-any.whl (9.2 MB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "     ---------------------------------------- 79.6/79.6 kB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\ikim1\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from botocore<1.28.0,>=1.27.89->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\ikim1\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from botocore<1.28.0,>=1.27.89->boto3) (1.26.12)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ikim1\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.89->boto3) (1.16.0)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.24.89 botocore-1.27.89 jmespath-1.0.1 s3transfer-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "900c08e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c12b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT PUSH CREDENTIALS TO REPO\n",
    "s3 = boto3.client(\n",
    "    service_name='s3',\n",
    "    region_name='us-west-2',\n",
    "    aws_access_key_id='',\n",
    "    aws_secret_access_key=''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa40ed06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "Successful S3 get_object response. Status - 200\n",
      "   Unnamed: 0           author  \\\n",
      "0     2099088  UghImRegistered   \n",
      "1     2893884  PossibleLesbian   \n",
      "2     2237635        [deleted]   \n",
      "3      498777          Naztash   \n",
      "4     1337130        BurChaBow   \n",
      "\n",
      "                                                body  \\\n",
      "0  &gt; “I have friends with the same degree as m...   \n",
      "1  Just a bit of background: I grew up Catholic. ...   \n",
      "2  I myself enjoy approaching an attractive young...   \n",
      "3  You do realize that the contract probably has ...   \n",
      "4  [](/dashiewilliamisboredofnamingemotes)\\n\\nI g...   \n",
      "\n",
      "                                      normalizedBody       subreddit  \\\n",
      "0  > “I have friends with the same degree as me, ...        politics   \n",
      "1  Just a bit of background: I grew up Catholic. ...  actuallesbians   \n",
      "2  I myself enjoy approaching an attractive young...       AskReddit   \n",
      "3  You do realize that the contract probably has ...         TopGear   \n",
      "4  \\n I got a teacher that used the most ridiculo...       MLPLounge   \n",
      "\n",
      "  subreddit_id         id                                            content  \\\n",
      "0     t5_2cneq    c1v1gu3  I have friends with the same degree as me, fro...   \n",
      "1     t5_2rch0  t3_185lqh  Just a bit of background: I grew up Catholic. ...   \n",
      "2     t5_2qh1i   t3_g0rk6  I myself enjoy approaching an attractive young...   \n",
      "3     t5_2r9n6    cpcdljw  You do realize that the contract probably has ...   \n",
      "4     t5_2t403    cgej4pd  I got a teacher that used the most ridiculous ...   \n",
      "\n",
      "                                             summary  \n",
      "0                                   co-op, get some.  \n",
      "1  Former Catholic confused about sexuality. Has ...  \n",
      "2  I've noticed a lot of stuff on Reddit concerni...  \n",
      "3  He is not their child, but he is acting like a...  \n",
      "4  Teacher likes papers, and said  \"Pdf isn't the...  \n"
     ]
    }
   ],
   "source": [
    "# %time\n",
    "# response = s3.get_object(Bucket='reddit-title-generation', Key=\"dataset-train.csv\")\n",
    "\n",
    "# status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "# if status == 200:\n",
    "#     print(f\"Successful S3 get_object response. Status - {status}\")\n",
    "#     train = pd.read_csv(response.get(\"Body\"))\n",
    "#     print(train.head())\n",
    "# else:\n",
    "#     print(f\"Unsuccessful S3 get_object response. Status - {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66b6b98c-2615-49e7-ab0f-902edcbaa0f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "EventStreamError",
     "evalue": "An error occurred (InternalError) when calling the SelectObjectContent operation: We encountered an internal error. Please try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEventStreamError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 13\u001b[0m\n\u001b[0;32m      2\u001b[0m resp \u001b[38;5;241m=\u001b[39m s3\u001b[38;5;241m.\u001b[39mselect_object_content(\n\u001b[0;32m      3\u001b[0m     Bucket\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreddit-title-generation\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m     Key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset-train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     OutputSerialization \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCSV\u001b[39m\u001b[38;5;124m'\u001b[39m: {}},\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m records \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m resp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPayload\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecords\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m event:\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;66;03m# records.append(event['Records']['Payload'].decode('utf-8'))\u001b[39;00m\n\u001b[0;32m     16\u001b[0m         records\u001b[38;5;241m.\u001b[39mappend(event[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecords\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPayload\u001b[39m\u001b[38;5;124m'\u001b[39m])  \n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\botocore\\eventstream.py:603\u001b[0m, in \u001b[0;36mEventStream.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_generator:\n\u001b[1;32m--> 603\u001b[0m         parsed_event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    604\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m parsed_event:\n\u001b[0;32m    605\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m parsed_event\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\botocore\\eventstream.py:619\u001b[0m, in \u001b[0;36mEventStream._parse_event\u001b[1;34m(self, event)\u001b[0m\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 619\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EventStreamError(parsed_response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_operation_name)\n",
      "\u001b[1;31mEventStreamError\u001b[0m: An error occurred (InternalError) when calling the SelectObjectContent operation: We encountered an internal error. Please try again."
     ]
    }
   ],
   "source": [
    "# Querying CSV for relevant results\n",
    "resp = s3.select_object_content(\n",
    "    Bucket='reddit-title-generation',\n",
    "    Key='dataset-train.csv',\n",
    "    ExpressionType='SQL',\n",
    "    # Need to use positional headers in query\n",
    "    Expression=\"SELECT * FROM s3object s WHERE CHAR_LENGTH(s._2) < 100 LIMIT 10000\",\n",
    "    InputSerialization = {'CSV': {\"FileHeaderInfo\": \"NONE\", 'AllowQuotedRecordDelimiter':True}, 'CompressionType': 'NONE'},\n",
    "    OutputSerialization = {'CSV': {}},\n",
    ")\n",
    "\n",
    "records = []\n",
    "for event in resp['Payload']:\n",
    "    if 'Records' in event:\n",
    "        # records.append(event['Records']['Payload'].decode('utf-8'))\n",
    "        records.append(event['Records']['Payload'])  \n",
    "        \n",
    "file_str = ''.join(req.decode('utf-8') for req in records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b70f2c",
   "metadata": {},
   "source": [
    "Maximum length per row in data is 1MB, which I believe is causing issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecc7521-9b89-41d9-9bd4-6b708e0477dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "train = pd.read_csv(StringIO(file_str), header=0)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba3913c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7358264d-72d1-4a76-93f7-3b4babc2fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "from transformers import *\n",
    "from blurr.text.data.all import *\n",
    "from blurr.text.modeling.all import *\n",
    "\n",
    "#Select part of data we want to keep\n",
    "train_texts = train[['content','summary']]\n",
    "\n",
    "#Clean text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef961e7",
   "metadata": {},
   "source": [
    "## BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc0c6fdb-b2a4-4bbc-9aea-d0e2aae185be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\blurr\\text\\data\\seq2seq\\summarization.py:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  final_df = final_df.append(self._process_df_batch(batch_df))\n",
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\blurr\\text\\data\\seq2seq\\summarization.py:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  final_df = final_df.append(self._process_df_batch(batch_df))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to IPython and Windows limitation, python multiprocessing isn't available now.\n",
      "So `number_workers` is changed to 0 to avoid getting stuck\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_name = \"facebook/bart-large-cnn\"\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(pretrained_model_name, \n",
    "                                                                  model_cls=BartForConditionalGeneration)\n",
    "\n",
    "# Create mini-batch and define parameters\n",
    "hf_batch_tfm = Seq2SeqBatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model, \n",
    "    task='summarization')\n",
    "\n",
    "# Simple preprocessing\n",
    "preprocessor = SummarizationPreprocessor(\n",
    "    hf_tokenizer,\n",
    "    text_attr='content',\n",
    "    target_text_attr='summary',\n",
    "    max_input_tok_length=256,\n",
    "    max_target_tok_length=130,\n",
    "    min_summary_char_length=30,\n",
    ")\n",
    "\n",
    "preprocessed_train = preprocessor.process_df(train_texts)\n",
    "\n",
    "\n",
    "# Prepare data for training\n",
    "blocks = (Seq2SeqTextBlock(batch_tokenize_tfm=hf_batch_tfm), noop)\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader('content'), get_y=ColReader('summary'), splitter=RandomSplitter())\n",
    "dls = dblock.dataloaders(preprocessed_train, bs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c5f4f06-9e5c-4885-94a0-8d6ffefe210d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6c3e21ab6447439d03ce3ca5827763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05537b9516b141f89aee82d16016d588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\torch\\amp\\autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>bertscore_precision</th>\n",
       "      <th>bertscore_recall</th>\n",
       "      <th>bertscore_f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.342315</td>\n",
       "      <td>3.172942</td>\n",
       "      <td>0.202566</td>\n",
       "      <td>0.047688</td>\n",
       "      <td>0.139865</td>\n",
       "      <td>0.653562</td>\n",
       "      <td>0.695616</td>\n",
       "      <td>0.673121</td>\n",
       "      <td>2:57:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.924973</td>\n",
       "      <td>3.115966</td>\n",
       "      <td>0.209516</td>\n",
       "      <td>0.053569</td>\n",
       "      <td>0.148705</td>\n",
       "      <td>0.654510</td>\n",
       "      <td>0.695004</td>\n",
       "      <td>0.673398</td>\n",
       "      <td>2:21:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.240080</td>\n",
       "      <td>3.207419</td>\n",
       "      <td>0.211030</td>\n",
       "      <td>0.053737</td>\n",
       "      <td>0.148333</td>\n",
       "      <td>0.655003</td>\n",
       "      <td>0.697316</td>\n",
       "      <td>0.674661</td>\n",
       "      <td>2:36:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\transformers\\generation_utils.py:1227: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 142 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f262e5791404cde877bef8ae6a52681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55a1e133cc248d3a9812daedeb3cabb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b843a1f7e24445589a4a545263b734fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e0f7ca196d496bb2797b96ae110980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Define performance metrics\n",
    "seq2seq_metrics = {\n",
    "        'rouge': {\n",
    "            'compute_kwargs': { 'rouge_types': [\"rouge1\", \"rouge2\", \"rougeL\"], 'use_stemmer': True },\n",
    "            'returns': [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    "        },\n",
    "        'bertscore': {\n",
    "            'compute_kwargs': { 'lang': 'fr' },\n",
    "            'returns': [\"precision\", \"recall\", \"f1\"]}}\n",
    "\n",
    "#Model\n",
    "model = BaseModelWrapper(hf_model)\n",
    "learn_cbs = [BaseModelCallback]\n",
    "fit_cbs = [Seq2SeqMetricsCallback(custom_metrics=seq2seq_metrics)]\n",
    "\n",
    "#Specify training\n",
    "learn = Learner(dls, model,\n",
    "                opt_func=ranger,loss_func=CrossEntropyLossFlat(),\n",
    "                cbs=learn_cbs,splitter=partial(blurr_seq2seq_splitter, arch=hf_arch)).to_fp16()\n",
    "\n",
    "#Create optimizer with default hyper-parameters\n",
    "learn.create_opt() \n",
    "learn.freeze()\n",
    "\n",
    "#Training\n",
    "learn.fit_one_cycle(3, lr_max=3e-5, cbs=fit_cbs)\n",
    "\n",
    "#Exporting model\n",
    "learn.save('reddit_summary.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23a98ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('models/reddit_summary.pkl.pth')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exporting model\n",
    "learn.save('reddit_summary.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3de6403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Loading model\n",
    "# learn.load('reddit_summary.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "479dee21-f75c-454b-a52e-65b4b0b0f31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prediction 1 ===\n",
      "{'generated_texts': \" co-op programs can make you more marketable, not less marketable.  It's not just about luck, it's about networking, experience and professionalism.  If you want to get out of your rut,\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = learn.blurr_generate(train_texts.iloc[0].content, early_stopping=False, num_return_sequences=1, \\\n",
    "                               min_length=30, max_length=50)\n",
    "\n",
    "for idx, o in enumerate(outputs):\n",
    "    print(f'=== Prediction {idx+1} ===\\n{o}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63aed238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I have friends with the same degree as me, from a worse school, but because of who they knew or when they happened to graduate, they’re in much better jobs,” said Kyle Bishop, 23, a 2009 graduate of the University of Pittsburgh who has spent the last two years waiting tables, delivering beer, working at a bookstore and entering data. “It’s more about luck than anything else.” \\n It's not  just  about luck, Kyle.  It's also about networking, experience and professionalism. \\n This is why you should, if you have the opportunity, enrol in a co-op program.  I graduated with 2 full years of real-world work experience in my field and had a job lined up before I graduated.  In fact, I still don't technically have my degree because I was a lazy student and had to take some part time courses while I worked full time.  But that hasn't stopped me from already being hired for two jobs that pay 65k+ during the tail end of a recession.  At the end of the day they hire me because I can prove my effectiveness as an employee, not just as a student. \\n Granted my experience comes from an in-demand field.  But when I compare myself to others who have taken more schooling in the same field but without coop, I'm WAY more marketable.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts.iloc[0].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dd5d95",
   "metadata": {},
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6b8cd318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed9b139121e4f2cb5a45976456411cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3c6ea0d3c04ffa84ccacea6cfe4c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19afb4c55d75425a81a797cf4b9bf513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7e1f07d64844689d837f8c41fbd251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\blurr\\text\\data\\seq2seq\\summarization.py:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  final_df = final_df.append(self._process_df_batch(batch_df))\n",
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\blurr\\text\\data\\seq2seq\\summarization.py:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  final_df = final_df.append(self._process_df_batch(batch_df))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to IPython and Windows limitation, python multiprocessing isn't available now.\n",
      "So `number_workers` is changed to 0 to avoid getting stuck\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_name = \"t5-base\"\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(pretrained_model_name, \n",
    "                                                                  model_cls=T5ForConditionalGeneration)\n",
    "\n",
    "# Create mini-batch and define parameters\n",
    "hf_batch_tfm = Seq2SeqBatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model, \n",
    "    task='summarization')\n",
    "\n",
    "# Simple preprocessing\n",
    "preprocessor = SummarizationPreprocessor(\n",
    "    hf_tokenizer,\n",
    "    text_attr='content',\n",
    "    target_text_attr='summary',\n",
    "    max_input_tok_length=256,\n",
    "    max_target_tok_length=130,\n",
    "    min_summary_char_length=30,\n",
    ")\n",
    "\n",
    "preprocessed_train = preprocessor.process_df(train_texts)\n",
    "\n",
    "\n",
    "# Prepare data for training\n",
    "blocks = (Seq2SeqTextBlock(batch_tokenize_tfm=hf_batch_tfm), noop)\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader('content'), get_y=ColReader('summary'), splitter=RandomSplitter())\n",
    "dls = dblock.dataloaders(preprocessed_train, bs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e0712091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define performance metrics\n",
    "seq2seq_metrics = {\n",
    "        'rouge': {\n",
    "            'compute_kwargs': { 'rouge_types': [\"rouge1\", \"rouge2\", \"rougeL\"], 'use_stemmer': True },\n",
    "            'returns': [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    "        },\n",
    "        'bertscore': {\n",
    "            'compute_kwargs': { 'lang': 'fr' },\n",
    "            'returns': [\"precision\", \"recall\", \"f1\"]}}\n",
    "\n",
    "#Model\n",
    "model = BaseModelWrapper(hf_model)\n",
    "learn_cbs = [BaseModelCallback]\n",
    "fit_cbs = [Seq2SeqMetricsCallback(custom_metrics=seq2seq_metrics)]\n",
    "\n",
    "#Specify training\n",
    "learn = Learner(dls, model,\n",
    "                opt_func=ranger,loss_func=CrossEntropyLossFlat(),\n",
    "                cbs=learn_cbs,splitter=partial(blurr_seq2seq_splitter, arch=hf_arch)).to_fp16()\n",
    "\n",
    "#Create optimizer with default hyper-parameters\n",
    "learn.create_opt() \n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85194498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('models/reddit_summary.pkl.pth')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exporting model\n",
    "learn.save('reddit_summary.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "456398d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\torch\\amp\\autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>bertscore_precision</th>\n",
       "      <th>bertscore_recall</th>\n",
       "      <th>bertscore_f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.803366</td>\n",
       "      <td>3.810098</td>\n",
       "      <td>0.126422</td>\n",
       "      <td>0.020688</td>\n",
       "      <td>0.103389</td>\n",
       "      <td>0.654869</td>\n",
       "      <td>0.623079</td>\n",
       "      <td>0.637797</td>\n",
       "      <td>1:09:34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\transformers\\generation_utils.py:1227: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Path('models/t5_reddit_summary.pkl.pth')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training\n",
    "learn.fit_one_cycle(1, lr_max=3e-5, cbs=fit_cbs)\n",
    "\n",
    "#Exporting model\n",
    "learn.save('t5_reddit_summary.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "49c62a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prediction 1 ===\n",
      "{'generated_texts': ['I graduated with two full years of real-world work experience in my field . But the things I learned are a lot harder for me to overcome .', 'the cost of coop is high and the benefits are huge. What are some co-op ways to save money on college? coop is much more about people having a second career and coop is about professional development as well as', 'the result is more potential employer value than any other degree you could have earned. “I work in tech, but my job is not in business.”']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = learn.blurr_generate(train_texts.iloc[0].content, early_stopping=True, \\\n",
    "                               # Exploring decoding strategies\n",
    "                               do_sample=True, top_k=50,top_p=0.95,\\\n",
    "                               min_length=30, max_length=50, num_return_sequences=3)\n",
    "\n",
    "for idx, o in enumerate(outputs):\n",
    "    print(f'=== Prediction {idx+1} ===\\n{o}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "990d929c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I have friends with the same degree as me, from a worse school, but because of who they knew or when they happened to graduate, they’re in much better jobs,” said Kyle Bishop, 23, a 2009 graduate of the University of Pittsburgh who has spent the last two years waiting tables, delivering beer, working at a bookstore and entering data. “It’s more about luck than anything else.” \\n It's not  just  about luck, Kyle.  It's also about networking, experience and professionalism. \\n This is why you should, if you have the opportunity, enrol in a co-op program.  I graduated with 2 full years of real-world work experience in my field and had a job lined up before I graduated.  In fact, I still don't technically have my degree because I was a lazy student and had to take some part time courses while I worked full time.  But that hasn't stopped me from already being hired for two jobs that pay 65k+ during the tail end of a recession.  At the end of the day they hire me because I can prove my effectiveness as an employee, not just as a student. \\n Granted my experience comes from an in-demand field.  But when I compare myself to others who have taken more schooling in the same field but without coop, I'm WAY more marketable.\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts.iloc[0].content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
