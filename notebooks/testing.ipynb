{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89ac5c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6595b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT PUSH CREDENTIALS TO REPO\n",
    "s3 = boto3.client(\n",
    "    service_name='s3',\n",
    "    region_name='us-west-2',\n",
    "    aws_access_key_id='',\n",
    "    aws_secret_access_key=''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3251b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying CSV for relevant results\n",
    "resp = s3.select_object_content(\n",
    "    Bucket='reddit-title-generation',\n",
    "    Key='dataset-train.csv',\n",
    "    ExpressionType='SQL',\n",
    "    # Need to use positional headers in query\n",
    "    Expression=\"SELECT * FROM s3object s LIMIT 1000\",\n",
    "    InputSerialization = {'CSV': {\"FileHeaderInfo\": \"NONE\", 'AllowQuotedRecordDelimiter':True}, 'CompressionType': 'NONE'},\n",
    "    OutputSerialization = {'CSV': {}},\n",
    ")\n",
    "\n",
    "records = []\n",
    "for event in resp['Payload']:\n",
    "    if 'Records' in event:\n",
    "        # records.append(event['Records']['Payload'].decode('utf-8'))\n",
    "        records.append(event['Records']['Payload'])  \n",
    "        \n",
    "file_str = ''.join(req.decode('utf-8') for req in records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90be4264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>normalizedBody</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2099088</td>\n",
       "      <td>UghImRegistered</td>\n",
       "      <td>&amp;gt; â€œI have friends with the same degree as m...</td>\n",
       "      <td>&gt; â€œI have friends with the same degree as me, ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>t5_2cneq</td>\n",
       "      <td>c1v1gu3</td>\n",
       "      <td>I have friends with the same degree as me, fro...</td>\n",
       "      <td>co-op, get some.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2893884</td>\n",
       "      <td>PossibleLesbian</td>\n",
       "      <td>Just a bit of background: I grew up Catholic. ...</td>\n",
       "      <td>Just a bit of background: I grew up Catholic. ...</td>\n",
       "      <td>actuallesbians</td>\n",
       "      <td>t5_2rch0</td>\n",
       "      <td>t3_185lqh</td>\n",
       "      <td>Just a bit of background: I grew up Catholic. ...</td>\n",
       "      <td>Former Catholic confused about sexuality. Has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2237635</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>I myself enjoy approaching an attractive young...</td>\n",
       "      <td>I myself enjoy approaching an attractive young...</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_g0rk6</td>\n",
       "      <td>I myself enjoy approaching an attractive young...</td>\n",
       "      <td>I've noticed a lot of stuff on Reddit concerni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>498777</td>\n",
       "      <td>Naztash</td>\n",
       "      <td>You do realize that the contract probably has ...</td>\n",
       "      <td>You do realize that the contract probably has ...</td>\n",
       "      <td>TopGear</td>\n",
       "      <td>t5_2r9n6</td>\n",
       "      <td>cpcdljw</td>\n",
       "      <td>You do realize that the contract probably has ...</td>\n",
       "      <td>He is not their child, but he is acting like a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1337130</td>\n",
       "      <td>BurChaBow</td>\n",
       "      <td>[](/dashiewilliamisboredofnamingemotes)\\n\\nI g...</td>\n",
       "      <td>ï¿¿ I got a teacher that used the most ridiculou...</td>\n",
       "      <td>MLPLounge</td>\n",
       "      <td>t5_2t403</td>\n",
       "      <td>cgej4pd</td>\n",
       "      <td>I got a teacher that used the most ridiculous ...</td>\n",
       "      <td>Teacher likes papers, and said  \"Pdf isn't the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0           author  \\\n",
       "0    2099088  UghImRegistered   \n",
       "1    2893884  PossibleLesbian   \n",
       "2    2237635        [deleted]   \n",
       "3     498777          Naztash   \n",
       "4    1337130        BurChaBow   \n",
       "\n",
       "                                                body  \\\n",
       "0  &gt; â€œI have friends with the same degree as m...   \n",
       "1  Just a bit of background: I grew up Catholic. ...   \n",
       "2  I myself enjoy approaching an attractive young...   \n",
       "3  You do realize that the contract probably has ...   \n",
       "4  [](/dashiewilliamisboredofnamingemotes)\\n\\nI g...   \n",
       "\n",
       "                                      normalizedBody       subreddit  \\\n",
       "0  > â€œI have friends with the same degree as me, ...        politics   \n",
       "1  Just a bit of background: I grew up Catholic. ...  actuallesbians   \n",
       "2  I myself enjoy approaching an attractive young...       AskReddit   \n",
       "3  You do realize that the contract probably has ...         TopGear   \n",
       "4  ï¿¿ I got a teacher that used the most ridiculou...       MLPLounge   \n",
       "\n",
       "  subreddit_id         id                                            content  \\\n",
       "0     t5_2cneq    c1v1gu3  I have friends with the same degree as me, fro...   \n",
       "1     t5_2rch0  t3_185lqh  Just a bit of background: I grew up Catholic. ...   \n",
       "2     t5_2qh1i   t3_g0rk6  I myself enjoy approaching an attractive young...   \n",
       "3     t5_2r9n6    cpcdljw  You do realize that the contract probably has ...   \n",
       "4     t5_2t403    cgej4pd  I got a teacher that used the most ridiculous ...   \n",
       "\n",
       "                                             summary  \n",
       "0                                   co-op, get some.  \n",
       "1  Former Catholic confused about sexuality. Has ...  \n",
       "2  I've noticed a lot of stuff on Reddit concerni...  \n",
       "3  He is not their child, but he is acting like a...  \n",
       "4  Teacher likes papers, and said  \"Pdf isn't the...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import StringIO\n",
    "train = pd.read_csv(StringIO(file_str), header=0)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3360b3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\torchaudio\\backend\\utils.py:62: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n",
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\blurr\\text\\modeling\\question_answering.py:31: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  squad_metric = load_metric(\"squad\")\n"
     ]
    }
   ],
   "source": [
    "from fastai.text.all import *\n",
    "from transformers import *\n",
    "from blurr.text.data.all import *\n",
    "from blurr.text.modeling.all import *\n",
    "\n",
    "#Select part of data we want to keep\n",
    "train_texts = train[['content','summary']]\n",
    "\n",
    "#Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d118d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\blurr\\text\\data\\seq2seq\\summarization.py:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  final_df = final_df.append(self._process_df_batch(batch_df))\n",
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\blurr\\text\\data\\seq2seq\\summarization.py:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  final_df = final_df.append(self._process_df_batch(batch_df))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to IPython and Windows limitation, python multiprocessing isn't available now.\n",
      "So `number_workers` is changed to 0 to avoid getting stuck\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikim1\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_name = \"facebook/bart-large-cnn\"\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(pretrained_model_name, \n",
    "                                                                  model_cls=BartForConditionalGeneration)\n",
    "\n",
    "# Create mini-batch and define parameters\n",
    "hf_batch_tfm = Seq2SeqBatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model, \n",
    "    task='summarization')\n",
    "\n",
    "# Simple preprocessing\n",
    "preprocessor = SummarizationPreprocessor(\n",
    "    hf_tokenizer,\n",
    "    text_attr='content',\n",
    "    target_text_attr='summary',\n",
    "    max_input_tok_length=256,\n",
    "    max_target_tok_length=130,\n",
    "    min_summary_char_length=30,\n",
    ")\n",
    "\n",
    "preprocessed_train = preprocessor.process_df(train_texts)\n",
    "\n",
    "\n",
    "# Prepare data for training\n",
    "blocks = (Seq2SeqTextBlock(batch_tokenize_tfm=hf_batch_tfm), noop)\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader('content'), get_y=ColReader('summary'), splitter=RandomSplitter())\n",
    "# Batch size can be changed here\n",
    "dls = dblock.dataloaders(preprocessed_train, bs = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06e19908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define performance metrics\n",
    "seq2seq_metrics = {\n",
    "        'rouge': {\n",
    "            'compute_kwargs': { 'rouge_types': [\"rouge1\", \"rouge2\", \"rougeL\"], 'use_stemmer': True },\n",
    "            'returns': [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    "        },\n",
    "        'bertscore': {\n",
    "            'compute_kwargs': { 'lang': 'fr' },\n",
    "            'returns': [\"precision\", \"recall\", \"f1\"]}}\n",
    "\n",
    "#Model\n",
    "model = BaseModelWrapper(hf_model)\n",
    "learn_cbs = [BaseModelCallback]\n",
    "fit_cbs = [Seq2SeqMetricsCallback(custom_metrics=seq2seq_metrics)]\n",
    "\n",
    "#Specify training\n",
    "learn = Learner(dls, model,\n",
    "                opt_func=ranger,loss_func=CrossEntropyLossFlat(),\n",
    "                cbs=learn_cbs,splitter=partial(blurr_seq2seq_splitter, arch=hf_arch)).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e971c29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = learn.load('bart_reddit_summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f01d08da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prediction 1 ===\n",
      "{'generated_texts': ' Kyle Bishop, 23, has spent the last two years waiting tables, delivering beer, working at a bookstore and entering data. â€œItâ€™s more about luck than anything else. I have friends with the same degree'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = learn.blurr_generate(train_texts.iloc[0].content, early_stopping=False, num_return_sequences=1, \\\n",
    "                               min_length=30, max_length=50)\n",
    "\n",
    "for idx, o in enumerate(outputs):\n",
    "    print(f'=== Prediction {idx+1} ===\\n{o}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
