{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yao\\Anaconda3\\lib\\site-packages\\blurr\\text\\modeling\\question_answering.py:31: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  squad_metric = load_metric(\"squad\")\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Yao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Yao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Yao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Yao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from fastai.text.all import *\n",
    "from fastai import *\n",
    "from blurr import *\n",
    "#from transformers import *\n",
    "import transformers\n",
    "from blurr.text.data.all import *\n",
    "from blurr.text.modeling.all import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import io\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from fastbook import load_learner\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!pip install fastbook\n",
    "##!pip install fastai\n",
    "#!pip install transformers\n",
    "##!pip install blurr\n",
    "#!pip install --upgrade h5py\n",
    "#!pip install libhdf5\n",
    "#print(h5py.__version__)\n",
    "#!pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers -Uqq\n",
    "#!pip install datasets -Uqq\n",
    "#!pip install bert-score -Uqq\n",
    "#!pip install sacremoses \n",
    "#!pip install ohmeow-blurr -Uqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('dataset-test.csv') # use test file just to load faster\n",
    "df_test = df_test[df_test['subreddit'] == 'leagueoflegends'] #subset\n",
    "test_texts = df_test[['content','summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train = pd.read_csv('dataset-train.csv')\n",
    "#train_texts = df_train[['content','summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(data):\n",
    "    stop_words = set(stopwords.words('english')) #use stop words\n",
    "    #data = re.sub('(\\d+)', '', data) #remove digits\n",
    "    data = re.sub('\\W+',' ', data) #remove special chaacters\n",
    "    word_tokens = word_tokenize(data.lower()) #tokenize the string\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words] #remove stop words\n",
    "    #porter_stemmer = PorterStemmer() #stem the words \n",
    "    #stemmed_words = [porter_stemmer.stem(word) for word in filtered_sentence]\n",
    "    wordnet_lemmatizer = WordNetLemmatizer() #lemm the word, does better than stem the word\n",
    "    lemm_words = [wordnet_lemmatizer.lemmatize(word) for word in filtered_sentence]\n",
    "    return ' '.join(lemm_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_rscore(reference, candidate): #assuming both are lists, Rouge\n",
    "    rscores = []\n",
    "    rscorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    for i in range(len(reference)):\n",
    "        rscore = rscorer.score(reference[i], candidate[i])\n",
    "        rscores.append(list(rscore.values())[0][2]) # fscore\n",
    "    return rscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yao\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3543: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  \"`as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to IPython and Windows limitation, python multiprocessing isn't available now.\n",
      "So `number_workers` is changed to 0 to avoid getting stuck\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yao\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3543: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  \"`as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your \"\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_name = \"facebook/bart-large-cnn\"\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(pretrained_model_name, \n",
    "                                                                  model_cls=transformers.BartForConditionalGeneration)\n",
    "\n",
    "# Create mini-batch and define parameters\n",
    "hf_batch_tfm = Seq2SeqBatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model, \n",
    "    task='summarization')\n",
    "\n",
    "# Simple preprocessing\n",
    "preprocessor = SummarizationPreprocessor(\n",
    "    hf_tokenizer,\n",
    "    text_attr='content',\n",
    "    target_text_attr='summary',\n",
    "    max_input_tok_length=256,\n",
    "    max_target_tok_length=130,\n",
    "    min_summary_char_length=30,\n",
    ")\n",
    "\n",
    "preprocessed_train = preprocessor.process_df(test_texts) #test_texts here\n",
    "\n",
    "\n",
    "# Prepare data for training\n",
    "blocks = (Seq2SeqTextBlock(batch_tokenize_tfm=hf_batch_tfm), noop)\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader('content'), get_y=ColReader('summary'), splitter=RandomSplitter())\n",
    "# Batch size can be changed here\n",
    "dls = dblock.dataloaders(preprocessed_train, bs = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define performance metrics\n",
    "seq2seq_metrics = {\n",
    "        'rouge': {\n",
    "            'compute_kwargs': { 'rouge_types': [\"rouge1\", \"rouge2\", \"rougeL\"], 'use_stemmer': True },\n",
    "            'returns': [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    "        },\n",
    "        'bertscore': {\n",
    "            'compute_kwargs': { 'lang': 'fr' },\n",
    "            'returns': [\"precision\", \"recall\", \"f1\"]}}\n",
    "\n",
    "#Model\n",
    "model = BaseModelWrapper(hf_model)\n",
    "learn_cbs = [BaseModelCallback]\n",
    "fit_cbs = [Seq2SeqMetricsCallback(custom_metrics=seq2seq_metrics)]\n",
    "\n",
    "#Specify training\n",
    "learn = Learner(dls, model,\n",
    "                opt_func=ranger,loss_func=CrossEntropyLossFlat(),\n",
    "                cbs=learn_cbs,splitter=partial(blurr_seq2seq_splitter, arch=hf_arch)).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('models/reddit_summary.pkl.pth')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save('reddit_summary.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = learn.load('reddit_summary.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inf_learn = load_learner('Reddit-Title-Generation/notebooks/reddit_summary.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prediction 1 ===\n",
      "{'generated_texts': 'Riot needs to stop trying to please all the new players begging for legacy content and start to honor the old players that have been investing in the game from the very beginning. If Riot puts them up for sale again they lose all value'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = learn.blurr_generate(test_texts.iloc[0].content, early_stopping=False, num_return_sequences=1, \\\n",
    "                               min_length=30, max_length=50)\n",
    "for idx, o in enumerate(outputs):\n",
    "    print(f'=== Prediction {idx+1} ===\\n{o}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$prediction$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('dataset-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = df_test[df_test['subreddit'] == 'AskReddit'].sample(50)[['content','summary']]\n",
    "df_l = df_test[df_test['subreddit'] == 'leagueoflegends'].sample(50)[['content','summary']]\n",
    "df_t = df_test[df_test['subreddit'] == 'tifu'].sample(50)[['content','summary']]\n",
    "df_r = df_test[df_test['subreddit'] == 'relationships'].sample(50)[['content','summary']]\n",
    "df_ra = df_test[df_test['subreddit'] == 'relationship_advice'].sample(50)[['content','summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing optional\n",
    "df_a['content'] = list(map(pre_processing, df_a['content'])) \n",
    "df_a['summary'] = list(map(pre_processing, df_a['summary'])) \n",
    "df_l['content'] = list(map(pre_processing, df_l['content'])) \n",
    "df_l['summary'] = list(map(pre_processing, df_l['summary'])) \n",
    "df_t['content'] = list(map(pre_processing, df_t['content'])) \n",
    "df_t['summary'] = list(map(pre_processing, df_t['summary'])) \n",
    "df_r['content'] = list(map(pre_processing, df_r['content'])) \n",
    "df_r['summary'] = list(map(pre_processing, df_r['summary'])) \n",
    "df_ra['content'] = list(map(pre_processing, df_ra['content'])) \n",
    "df_ra['summary'] = list(map(pre_processing, df_ra['summary'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I dunno, that's the way I feel. I wouldn't want to get more involved than I already was. And if the same situation was occurring with my wife - I'd expect the same. You're right, karma. Im not particularly one to be a bystander. In this case i would be. Liars get caught and once a cheater always a cheater. If it'd make you feel better - send her a letter. I don't think my sanity could handle bearing that news upon someone. But, after hearing  what the fuck is wrong with you  it's clear I'm in the minority. Thanks for the colorful language. I'm sorry you feel so strongly about my opinion. On a different note I can recall the triple-homicide that occurred in Vermont. It was a love triangle that ended with a hunting rifle. I tried my best to locate the article on google. I'm afraid it occurred in the late 90's so it might not have a net report. It was very upsetting for the community.\""
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a.iloc[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(test_texts):\n",
    "    #list(outputs[0].values())\n",
    "    predictions = []\n",
    "    for i in range(len(test_texts)):\n",
    "        outputs = learn.blurr_generate(test_texts.iloc[i].content, early_stopping=False, num_return_sequences=1, \\\n",
    "                                   min_length=30, max_length=50)\n",
    "        predictions.append(list(outputs[0].values())[0])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_p = make_predictions(df_a)\n",
    "l_p = make_predictions(df_l)\n",
    "t_p = make_predictions(df_t)\n",
    "r_p = make_predictions(df_r)\n",
    "ra_p = make_predictions(df_ra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_rouge(test_texts, predictions):\n",
    "    score_list = f_rscore(list(test_texts.summary), predictions)\n",
    "    return np.average(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11916190621829653"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_rouge(df_a, a_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11731240898370848"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_rouge(df_l, l_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14023053352398496"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_rouge(df_t, t_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16361195193043446"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_rouge(df_r, r_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15174874941464958"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_rouge(df_ra, ra_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.0),\n",
       " (5, 0.0),\n",
       " (6, 0.04166666666666667),\n",
       " (16, 0.041666666666666664),\n",
       " (19, 0.0),\n",
       " (21, 0.0),\n",
       " (23, 0.04545454545454545),\n",
       " (26, 0.0),\n",
       " (28, 0.038461538461538464),\n",
       " (32, 0.03636363636363636),\n",
       " (33, 0.04347826086956522),\n",
       " (36, 0.0),\n",
       " (37, 0.04),\n",
       " (47, 0.04)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_list = f_rscore(list(df_a.summary), a_p)\n",
    "[(i ,e) for i, e in enumerate(score_list) if e < 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.5217391304347825),\n",
       " (14, 0.23809523809523808),\n",
       " (24, 0.25),\n",
       " (39, 0.2285714285714286),\n",
       " (41, 0.2156862745098039),\n",
       " (45, 0.2181818181818182)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_list = f_rscore(list(df_a.summary), a_p)\n",
    "[(i ,e) for i, e in enumerate(score_list) if e >0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Example$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Not me, but a friend. \\n She\\'s an animator. She\\'s actually quite good. She got a booth at an anime convention to sell some things. We helped her by manning the booth and getting people to buy stuff. \\n She opened and manned the booth the entire first day, we would visit her to see how it was going, not to good. \\n The second day she texted us and said \"I figured out how to sell!\"\\nWhen I showed up to man the booth for my shift, there was barely anything left. She was wearing a bikini top and really short shorts.'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a.iloc[2].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anything will be sold if there are boobs involved.'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a.iloc[2].summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A friend got a booth at an anime convention to sell some things. We helped her by manning the booth and getting people to buy stuff. The second day she texted us and said \"I figured out how to sell!\"'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_p[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Example$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A most excellent'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a.iloc[5].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Example$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This story applies to your situation fairly well. \\n A few years ago, I started dating this girl I met in college. She was a few years younger than me. She was beautiful, long blonde hair, sexy smile and had that confidence that made her all the more attractive. \\n We started dating early in the first semester and became fairly serious pretty quickly. By the end of the school year, we had decided we would move into together for the summer and the following year, this was becoming a fairly serious relationship. I was very excited to take these next steps with her because she was unlike any other lady I had dated prior. \\n Well when we moved in together, she stopped putting any effort into herself at all. She stopped combing her hair, started wearing sweat pants and hoodies every day and generally just looked disheveled all the time. \\n I still loved her though because I loved her for who she was and not what she wore or looked like. \\n That was up until the day she stopped wearing makeup. Something felt oddly different about her but it wasn\\'t until we went out at a local coffee shop that I noticed what that was. As she was getting our order she looked at me and said, \"hey I am a bit short on change, do you have about three fiddy?\" \\n Well it was when she said that, that I noticed she was 500 feet tall and from the paleolithic era. Turns out the makeup had been hiding her true identity all along. So I told her, \"goddamn lochness monster, I ain\\'t giving you no three fiddy\". \\n We broke up that day.'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a.iloc[19].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sometimes makeup is essential in a relationship.'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a.iloc[19].summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"I loved her for who she was and not what she wore or looked like\" \"I noticed she was 500 feet tall and from the paleolithic era\" \"Goddamn lochness monster, I ain\\'t giving you no'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_p[19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Example$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My best friend teaches freshman english. After class one day, a note was left and a group of girls in the next class grabbed it, read it, and started giggling. My friend grabbed the note from the girls, and threw it away without reading it, trying to save the student (a quite and polite student), and probably herself, some humiliation. However, when the next day rolled around and the same girl left another note behind, it was fair game. The 14-year-old had left behind a note addressed to her boyfriend and it read: \"Hey, wanna come over after school for some sex and capri sun?\". Unsure of what Capri Sun had to do with sex, my friend decided to search for it on urbandictionary.com. \"Capri Sun\" means getting jizzed on the face after a blow job. She was never able to look at that girl the same.'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a.iloc[24].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'14-year old freshman wanted her boyfriend to come on her face.'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a.iloc[24].summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Capri Sun\" means getting jizzed on the face after a blow job. The 14-year-old had left behind a note addressed to her boyfriend and it read: \"Hey, wanna come over after school for some'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_p[24]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
